<<<<<<< HEAD
}
runexample <- FALSE
if (runexample) {
data(NewHavenResidential)
gpairs(NewHavenResidential)
}
data("Titanic")
help(rpart)
help("rpart")
attach(titanic_data)
titanic_data <- data("Titanic")
attach(titanic_data)
titanic_data <- data("Titanic")
attach(titanic_data)
titanic_data <- data("Titanic")
attach(titanic_data)
force(Titanic)
force(Titanic)
data.frame(data("Titanic"))
data(Titanic)
force(Titanic)
# Practice on the Titanic Data
library(titanic)
# Practice on the Titanic Data
install.packages("titanic")
library(titanic)
data(Titanic)
data(titanic)
data(titanic_train)
force(Titanic)
force(titanic_train)
attach(titanic_train)
View(titanic_train)
fit <- rpart(titanic_train$Survived ~ .)
library(rpart)
fit <- rpart(titanic_train$Survived ~ .)
fit <- rpart(titanic_train$Survived ~ ., data=titanic_train)
plot(fit)
tree <- rpart(titanic_train$Survived ~ ., data=titanic_train)
plot(fit)
help(ctree)
tree <- ctree((titanic_train$Survived ~ ., data=titanic_train)
tree <- ctree(titanic_train$Survived ~ ., data=titanic_train)
tree <- ctree(as.factor(titanic_train$Survived) ~ ., data=titanic_train)
library(gdata)
library("xlsx")
brooklyn<-read.xlsx(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheetIndex=1,startRow=5,header=TRUE)
View(brooklyn)
brooklyn<-read.xlsx(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheetIndex=1,startRow=5,header=TRUE)
# Clean the data to remove NA rows
brooklyn <- na.omit(brooklyn)
############## PCA Analysis on the Boston Dataset ##############
install.packages('MASS')
data(Boston, package="MASS")
# Read the documentation of Boston dataset in RStudio to understand the dataset
help(Boston)
# Principal Component Analysis
# the prcomp() fucntion computes the principal components and we have turned on scalling
# Read the documentation for prcompt() function in RStudio
help(prcomp)
pca_out <- prcomp(Boston,scale. = T)
# pca_out shows the loadings that used.
pca_out
plot(pca_out)
# plotting using the biplot()
# Read the documentation for biplot() function in RStudio
help(biplot)
biplot(pca_out, scale = 0)
boston_pc <- pca_out$x
boston_pc
# boston_pc has the Principal Components having the same number of rows in the original dataset
head(boston_pc)
summary(boston_pc)
# plotting the boston_pc using the a line in plot() functions
plot(boston_pc, type = "l")
# Perform Linear regression on our data
regression_model<-lm(logged_DF$LOG_SP~0+logged_DF$LOG_GSF+logged_DF$LOG_LSF+fctr_NEIGHBOR+fctr_BLDNG_CLSS)
library(gdata)
library("xlsx")
library(corrplot)
library(dplyr)
library(mltools)
library(data.table)
brooklyn<-read.xlsx(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheetIndex=1,startRow=5,header=TRUE)
# Add our factored data before randomly sampling the data
logged_DF$fctr_NEIGHBOR <- fctr_NEIGHBOR
# Select the data and split utilizing a REGEX
SALE.PRICE<-sub("\\$","",SALE.PRICE)
# Clean the data to remove NA rows
brooklyn <- na.omit(brooklyn)
attach(brooklyn)
# Select the data and split utilizing a REGEX
SALE.PRICE<-sub("\\$","",SALE.PRICE)
SALE.PRICE<-as.numeric(gsub(",","", SALE.PRICE))
GROSS.SQUARE.FEET<-as.numeric(gsub(",","", GROSS.SQUARE.FEET))
LAND.SQUARE.FEET<-as.numeric(gsub(",","", LAND.SQUARE.FEET))
plot(log(GROSS.SQUARE.FEET), log(SALE.PRICE))
# Compute the logs of the necessary data and clean the values
log_sales_price <- log(SALE.PRICE)
log_GSF <- log(GROSS.SQUARE.FEET)
log_LSF <- log(LAND.SQUARE.FEET)
logged_DF <- data.frame(
"LOG_SP" = log_sales_price,
"LOG_GSF" = log_GSF,
"LOG_LSF" = log_LSF
)
logged_DF <- na.omit(logged_DF)
fctr_NEIGHBOR <- factor(brooklyn$NEIGHBORHOOD)
fctr_NEIGHBOR <- fctr_NEIGHBOR[!is.infinite(rowSums(logged_DF))]
fctr_BLDNG_CLSS <- factor(brooklyn$BUILDING.CLASS.CATEGORY)
fctr_BLDNG_CLSS <- fctr_BLDNG_CLSS[!is.infinite(rowSums(logged_DF))]
# Perform One hot encoding
neighborhood <- one_hot(as.data.table(fctr_NEIGHBOR))
blding_class <- one_hot(as.data.table(fctr_BLDNG_CLSS))
logged_DF <- logged_DF[!is.infinite(rowSums(logged_DF)),]
attach(logged_DF)
##################### Plot the correlation matrix  #####################
logged_DF.cor <- cor(logged_DF)
# Add our factored data before randomly sampling the data
logged_DF$fctr_NEIGHBOR <- fctr_NEIGHBOR
logged_DF$fctr_BLDNG_CLSS <- fctr_BLDNG_CLSS
dt = sort(sample(nrow(logged_DF), nrow(logged_DF)*.7))
train<-logged_DF[dt,]
test<-logged_DF[-dt,]
# Perform Linear regression on our data
regression_model<-lm(logged_DF$LOG_SP~0+logged_DF$LOG_GSF+logged_DF$LOG_LSF+logged_DF$fctr_NEIGHBOR+logged_DF$fctr_BLDNG_CLSS)
summary(regression_model)
plot(resid(regression_model))
# Perform Linear regression on our data
regression_model<-lm(train$LOG_SP~0+train$LOG_GSF+train$LOG_LSF+train$fctr_NEIGHBOR+train$fctr_BLDNG_CLSS)
summary(regression_model)
plot(resid(regression_model))
# Run our predictions on the test data and compare the results
predict(regression_model, newdata = test)
View(test)
View(train)
# Perform Linear regression on our data
regression_model<-lm(train$LOG_SP~0+train$LOG_GSF+train$LOG_LSF+train$fctr_NEIGHBOR+train$fctr_BLDNG_CLSS, data=train)
summary(regression_model)
plot(resid(regression_model))
# Run our predictions on the test data and compare the results
predict(regression_model, newdata = test)
# Perform Linear regression on our data
regression_model<-lm(LOG_SP~0+LOG_GSF+LOG_LSF+fctr_NEIGHBOR+fctr_BLDNG_CLSS, data=train)
summary(regression_model)
plot(resid(regression_model))
# Run our predictions on the test data and compare the results
predict(regression_model, newdata = test)
library(ggplot2)
# Run our predictions on the test data and compare the results
predictions <- predict(regression_model, newdata = test)
plot(predictions, test$LOG_SP, xlab="predicted", ylab="actual")
abline(a=0,b=1)
# Perform Linear regression on our data
regression_model<-lm(LOG_SP~0+LOG_GSF+LOG_LSF+fctr_NEIGHBOR*fctr_BLDNG_CLSS, data=train)
summary(regression_model)
plot(resid(regression_model))
# Plot our predicted vs actual with a reference line to gauge our model
plot(predictions, test$LOG_SP, xlab="predicted", ylab="actual")
# Run our predictions on the test data and compare the results
predictions <- predict(regression_model, newdata = test)
abline(a=0,b=1)
corrplot(logged_DF.cor, method = "circle")
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
fisher.test(contingencyMatrix, alternative = "greater")
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
fisher.test(logged_DF, alternative = "greater")
View(logged_DF)
library(gdata)
library("xlsx")
library(corrplot)
library(dplyr)
library(mltools)
library(data.table)
library(ggplot2)
###############################################################################
#               Read and Clean the Data                                       #
###############################################################################
brooklyn<-read.xlsx(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheetIndex=1,startRow=5,header=TRUE)
View(brooklyn)
# Clean the data to remove NA rows
brooklyn <- na.omit(brooklyn)
attach(brooklyn)
# Select the data and split utilizing a REGEX
SALE.PRICE<-sub("\\$","",SALE.PRICE)
SALE.PRICE<-as.numeric(gsub(",","", SALE.PRICE))
GROSS.SQUARE.FEET<-as.numeric(gsub(",","", GROSS.SQUARE.FEET))
LAND.SQUARE.FEET<-as.numeric(gsub(",","", LAND.SQUARE.FEET))
# Compute the logs of the necessary data and clean the values
log_sales_price <- log(SALE.PRICE)
log_GSF <- log(GROSS.SQUARE.FEET)
log_LSF <- log(LAND.SQUARE.FEET)
logged_DF <- data.frame(
"LOG_SP" = log_sales_price,
"LOG_GSF" = log_GSF,
"LOG_LSF" = log_LSF
)
logged_DF <- na.omit(logged_DF)
# Turn some of the miscellaneous but interesting columns into factors
fctr_NEIGHBOR <- factor(brooklyn$NEIGHBORHOOD)
fctr_NEIGHBOR <- fctr_NEIGHBOR[!is.infinite(rowSums(logged_DF))]
fctr_BLDNG_CLSS <- factor(brooklyn$BUILDING.CLASS.CATEGORY)
fctr_BLDNG_CLSS <- fctr_BLDNG_CLSS[!is.infinite(rowSums(logged_DF))]
logged_DF <- logged_DF[!is.infinite(rowSums(logged_DF)),]
attach(logged_DF)
###############################################################################
#               Plot a correlation matrix                                     #
###############################################################################
logged_DF.cor <- cor(logged_DF)
corrplot(logged_DF.cor, method = "circle")
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
fisher.test(logged_DF, alternative = "greater")
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
sp_v_gsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_GSF)
sp_v_lsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_LSF)
lsf_v_gsf <- t.test(logged_DF$LOG_GSF, logged_DF$LOG_LSF)
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
sp_v_gsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_GSF,  paired = TRUE)
sp_v_lsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_LSF,  paired = TRUE)
lsf_v_gsf <- t.test(logged_DF$LOG_GSF, logged_DF$LOG_LSF,  paired = TRUE)
if sp
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
T_TEST_THRESHOLD <- 0.05
sp_v_gsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_GSF,  paired = TRUE)
if (sp_v_gsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Sales Prices) is different from the mean weight of log(Gross Square Feet)")
} else {
print("The mean weight of log(Sales Prices) is similar from the mean weight of log(Gross Square Feet)")
}
if (sp_v_lsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Sales Prices) is different from the mean weight of log(Land Square Feet)")
} else {
print("The mean weight of log(Sales Prices) is similar from the mean weight of log(Land Square Feet)")
}
lsf_v_gsf <- t.test(logged_DF$LOG_GSF, logged_DF$LOG_LSF,  paired = TRUE)
if (lsf_v_gsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Land Square Feet) is different from the mean weight of log(Gross Square Feet)")
} else {
print("The mean weight of log(Land Square Feet) is similar from the mean weight of log(Gross Square Feet)")
}
summary(regression_model)
# Perform Linear regression on our data
regression_model<-lm(LOG_SP~0+LOG_GSF+LOG_LSF+fctr_NEIGHBOR+fctr_BLDNG_CLSS, data=train)
# Compute the logs of the necessary data and clean the values
log_sales_price <- log(SALE.PRICE)
log_GSF <- log(GROSS.SQUARE.FEET)
log_LSF <- log(LAND.SQUARE.FEET)
logged_DF <- data.frame(
"LOG_SP" = log_sales_price,
"LOG_GSF" = log_GSF,
"LOG_LSF" = log_LSF
)
logged_DF <- na.omit(logged_DF)
# Turn some of the miscellaneous but interesting columns into factors
fctr_NEIGHBOR <- factor(brooklyn$NEIGHBORHOOD)
fctr_NEIGHBOR <- fctr_NEIGHBOR[!is.infinite(rowSums(logged_DF))]
fctr_BLDNG_CLSS <- factor(brooklyn$BUILDING.CLASS.CATEGORY)
fctr_BLDNG_CLSS <- fctr_BLDNG_CLSS[!is.infinite(rowSums(logged_DF))]
logged_DF <- logged_DF[!is.infinite(rowSums(logged_DF)),]
attach(logged_DF)
###############################################################################
#               Plot a correlation matrix                                     #
###############################################################################
logged_DF.cor <- cor(logged_DF)
corrplot(logged_DF.cor, method = "circle")
###############################################################################
#             Perform a significance test on the variables                    #
###############################################################################
T_TEST_THRESHOLD <- 0.05
sp_v_gsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_GSF,  paired = TRUE)
=======
>>>>>>> 3d531c7... Add new files for SVM Labs
if (sp_v_gsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Sales Prices) is different from the mean weight of log(Gross Square Feet)")
} else {
print("The mean weight of log(Sales Prices) is similar from the mean weight of log(Gross Square Feet)")
}
sp_v_lsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_LSF,  paired = TRUE)
if (sp_v_lsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Sales Prices) is different from the mean weight of log(Land Square Feet)")
} else {
print("The mean weight of log(Sales Prices) is similar from the mean weight of log(Land Square Feet)")
}
lsf_v_gsf <- t.test(logged_DF$LOG_GSF, logged_DF$LOG_LSF,  paired = TRUE)
if (lsf_v_gsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Land Square Feet) is different from the mean weight of log(Gross Square Feet)")
} else {
print("The mean weight of log(Land Square Feet) is similar from the mean weight of log(Gross Square Feet)")
}
# Add our factored data before randomly sampling the data
logged_DF$fctr_NEIGHBOR <- fctr_NEIGHBOR
logged_DF$fctr_BLDNG_CLSS <- fctr_BLDNG_CLSS
dt = sort(sample(nrow(logged_DF), nrow(logged_DF)*.7))
train<-logged_DF[dt,]
test<-logged_DF[-dt,]
# Perform Linear regression on our data
regression_model<-lm(LOG_SP~0+LOG_GSF+LOG_LSF+fctr_NEIGHBOR+fctr_BLDNG_CLSS, data=train)
summary(regression_model)
plot(resid(regression_model))
###############################################################################
#                   Test/Analyze the model                                    #
###############################################################################
# Run our predictions on the test data and compare the results
predictions <- predict(regression_model, newdata = test)
# Plot our predicted vs actual with a reference line to gauge our model
plot(predictions, test$LOG_SP, xlab="predicted", ylab="actual")
abline(a=0,b=1)
if (sp_v_gsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Sales Prices) is different from the mean weight of log(Gross Square Feet)")
} else {
print("The mean weight of log(Sales Prices) is similar from the mean weight of log(Gross Square Feet)")
}
sp_v_lsf <- t.test(logged_DF$LOG_SP, logged_DF$LOG_LSF,  paired = TRUE)
if (sp_v_lsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Sales Prices) is different from the mean weight of log(Land Square Feet)")
} else {
print("The mean weight of log(Sales Prices) is similar from the mean weight of log(Land Square Feet)")
}
lsf_v_gsf <- t.test(logged_DF$LOG_GSF, logged_DF$LOG_LSF,  paired = TRUE)
if (lsf_v_gsf["p.value"] < T_TEST_THRESHOLD) {
print("The mean weight of log(Land Square Feet) is different from the mean weight of log(Gross Square Feet)")
} else {
print("The mean weight of log(Land Square Feet) is similar from the mean weight of log(Gross Square Feet)")
}
help(hclust)
help(dist)
cluster <- hclust(dist(Titanic))
plot(cluster)
cluster <- hclust(dist(Titanic$Survived))
cluster <- hclust(dist(Titanic$Survived))
plot(cluster)
cluster <- hclust(dist(Titanic))
plot(cluster)
library(rpart)
library(titanic)
library(randomForest)
data(titanic_train)
attach(titanic_train)
tree <- rpart(titanic_train$Survived ~ ., data=titanic_train)
plot(fit)
tree <- rpart(titanic_train$Survived ~ ., data=titanic_train)
plot(fit)
plot(tree)
tree <- ctree(as.factor(titanic_train$Survived) ~ ., data=titanic_train)
tree <- rpart(titanic_train$Survived ~ ., data=titanic_train)
tree
help(ctree)
tree <- ctree(as.factor(titanic_train$Survived) ~ ., data=titanic_train)
plot(tree)
tree <- ctree(as.factor(titanic_train$Survived) ~ ., data=titanic_train)
cluster <- hclust(dist(Titanic))
plot(cluster)
model <- randomForest(factor(Survived) ~ ., data = Titanic, importance = TRUE)
model
model <- randomForest(Survived ~ ., data = Titanic, importance = TRUE)
model
model <- randomForest(Survived ~ ., data = Titanic, importance = TRUE)
model
model <- randomForest(Survived ~ ., data = Titanic, importance = TRUE)
model
# Practice on the Titanic Data
install.packages("party")
library(party)
tree <- ctree(as.factor(titanic_train$Survived) ~ ., data=titanic_train)
tree <- ctree(as.factor(Survived) ~ ., data=titanic_train)
tree <- rpart(titanic_train$Survived ~ ., data=titanic_train)
tree
tree <- ctree(as.factor(Survived) ~ ., data=titanic_train)
plot(tree)
tree <- ctree(as.factor(titanic_train$Survived) ~ ., data=titanic_train)
tree <- ctree(titanic_train$Survived ~ ., data=titanic_train)
tree <- ctree(titanic_train$Survived~PassengerId+Pclass+Age+SibSp+Parch+Fare, data=titanic_train)
plot(tree)
tree <- ctree(titanic_train$Survived~PassengerId+Survived+Pclass+Age+SibSp+Parch+Fare, data=titanic_train)
plot(tree)
cluster <- hclust(dist(titanic_train))
plot(cluster)
cluster <- hclust(dist(titanic_train$Survived))
plot(cluster)
model <- randomForest(Survived ~ ., data = titanic_train, importance = TRUE)
model
cluster <- hclust(dist(titanic_train$Survived))
plot(cluster)
cluster <- hclust(dist(titanic_train))
plot(cluster)
cluster <- hclust(dist(titanic_train$Survived~.))
cluster <- hclust(dist(titanic_train$Survived))
plot(cluster)
matrix <- dist(titanic_train$Survived)
matrix
cluster <- hclust(matrix)
plot(cluster)
matrix <- dist(titanic_train)
matrix
cluster <- hclust(matrix)
plot(cluster)
# There are 13 variables in the dataset such as Alcohol, Malic Acid, Ash, Alkalinity of Ash, Magnesium, ...
wine_data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep = ",")
head(wine_data)
# Peek at the data
head(wine_data)
nrow(wine_data)
# Noticed the colnames are missing and thus, we need to add them
# Adding the variable names
colnames(wine_data) <- c("Cvs", "Alcohol",
"Malic_Acid", "Ash", "Alkalinity_of_Ash",
"Magnesium", "Total_Phenols", "Flavanoids", "NonFlavanoid_Phenols",
"Proanthocyanins", "Color_Intensity", "Hue", "OD280/OD315_of_Diluted_Wine",
"Proline")
head(wine_data) # Now you can see the header names.
# Using the Heatmap() function, we can check the correlations,
# In the heatmap(), the "Dark Colors" represent the "Correlated"
# In the heatmap(), the "Light Colors" represent the "Not Correlated"
# Now we will use the heatmap() function to show the correlation among variables.
heatmap(cor(wine_data),Rowv = NA, Colv = NA)
# declaring the cultivar_classes using the factor() function each cultivar Cv1,Cv2 and Cv3.
cultivar_classes <- factor(wine_data$Cvs)
cultivar_classes
# We will not normalize the Cvs variable (first column) so we exclude the Cvs column with with -1
wine_data_PCA <- prcomp(scale(wine_data[,-1]))
# Get a summary of the PCA
summary(wine_data_PCA)
# Plot the PCA analysis
plot(wine_data_PCA)
# Plot a L line graph
plot(wine_data_PCA, type = "l")
# Create a biplot
biplot(wine_data_PCA)
# There are 13 variables in the dataset such as Alcohol, Malic Acid, Ash, Alkalinity of Ash, Magnesium, ...
wine_data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep = ",")
# Peek at the data
head(wine_data)
nrow(wine_data)
# Noticed the colnames are missing and thus, we need to add them
# Adding the variable names
colnames(wine_data) <- c("Cvs", "Alcohol",
"Malic_Acid", "Ash", "Alkalinity_of_Ash",
"Magnesium", "Total_Phenols", "Flavanoids", "NonFlavanoid_Phenols",
"Proanthocyanins", "Color_Intensity", "Hue", "OD280/OD315_of_Diluted_Wine",
"Proline")
head(wine_data) # Now you can see the header names.
# Using the Heatmap() function, we can check the correlations,
# In the heatmap(), the "Dark Colors" represent the "Correlated"
# In the heatmap(), the "Light Colors" represent the "Not Correlated"
# Now we will use the heatmap() function to show the correlation among variables.
heatmap(cor(wine_data),Rowv = NA, Colv = NA)
# declaring the cultivar_classes using the factor() function each cultivar Cv1,Cv2 and Cv3.
cultivar_classes <- factor(wine_data$Cvs)
cultivar_classes
# We will not normalize the Cvs variable (first column) so we exclude the Cvs column with with -1
wine_data_PCA <- prcomp(scale(wine_data[,-1]))
# Get a summary of the PCA
summary(wine_data_PCA)
# Plot the PCA analysis
plot(wine_data_PCA)
# Plot the PCA analysis
plot(wine_data_PCA)
# Plot a L line graph
plot(wine_data_PCA, type = "l")
# Create a biplot
biplot(wine_data_PCA)
library(rpart)
library(titanic)
library(randomForest)
library(party)
data(titanic_train)
attach(titanic_train)
part <- rpart(titanic_train$Survived ~ ., data=titanic_train)
part
plot(part)
part
tree <- ctree(titanic_train$Survived~PassengerId+Pclass+Age+SibSp+Parch+Fare, data=titanic_train)
plot(tree)
matrix <- dist(titanic_train$Survived)
matrix
cluster <- hclust(matrix)
plot(cluster)
model <- randomForest(Survived ~ ., data = titanic_train, importance = TRUE)
model <- randomForest(Survived ~ ., data = titanic_train, importance = TRUE)
model <- randomForest(titanic_train$Survived ~ ., data = titanic_train, importance = TRUE)
attach(titanic_train)
model <- randomForest(Survived ~ ., data = titanic_train, importance = TRUE)
model
titanic_train <- na.omit(titanic_train)
model <- randomForest(Survived ~ ., data = titanic_train, importance = TRUE)
model
data(titanic_train)
attach(titanic_train)
model <- randomForest(Survived ~ ., data = titanic_train)
titanic_train <- na.omit(titanic_train)
model <- randomForest(Survived ~ ., data = titanic_train)
model <- randomForest(as.factor(Survived) ~ ., data = titanic_train)
model
#titanic_train <- na.omit(titanic_train)
model <- randomForest(as.factor(Survived) ~ ., data = titanic_train)
model
data(titanic_train)
attach(titanic_train)
#titanic_train <- na.omit(titanic_train)
model <- randomForest(as.factor(Survived) ~ ., data = titanic_train)
model <- randomForest(as.factor(Survived) ~ ., data = titanic_train)
model
<<<<<<< HEAD
=======
library(e1071)
set.seed(1)
x=matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1
x
y
# We begin by checking whether the classes are linearly separable.
plot(x, col=(3-y))
# They are not. Next, we fit the support vector classifier.
# We now create a data frame with the response coded as a factor.
dat <- data.frame(x = x,y  = as.factor(y))
svmfit <- svm(y ~., data=dat, kernel="linear", cost=10,scale=FALSE)
# We can now plot the support vector classifier obtained:
plot(svmfit , dat)
svm$index
svmfit$index
# Obtain basic information of the support classifier fit
summary(svmfit)
# Try a cost parameter of .1 instead
svmfit <- svm(y ~., data=dat, kernel="linear", cost = 0.1, scale=FALSE)
plot(svmfit , dat)
svmfit$index
tune.out <- tune(svm, y ~.,data=dat,kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
# We can easily access the cross-validation errors for each of these models using the summary() command:
summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)
# The predict() function can be used to predict the class label on a set of test observations,
# at any given value of the cost parameter. We begin by generating a test data set.
xtest=matrix(rnorm(20*2), ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
testdat=data.frame(x=xtest, y=as.factor(ytest))
# Predict the class labels of these observations
ypred <-predict(bestmod ,testdat)
table(predict=ypred, truth=testdat$y)
svmfit <- svm(y~., data=dat, kernel="linear", cost=.01, scale=FALSE)
ypred=predict(svmfit ,testdat)
table(predict=ypred, truth=testdat$y)
# We first further separate the two classes in our simulated data so that they are linearly separable:
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
# Now the observations are just barely linearly separable.
# We fit the support vector classifier and plot the resulting hyperplane,
# using a very large value of cost so that no observations are misclassified.
dat=data.frame(x=x,y=as.factor(y))
svmfit <-svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit,dat)
# We now try a smaller value of cost:
svmfit <- svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit ,dat)
# Now the observations are just barely linearly separable.
# We fit the support vector classifier and plot the resulting hyperplane,
# using a very large value of cost so that no observations are misclassified.
dat=data.frame(x=x,y=as.factor(y))
svmfit <-svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit,dat)
svmfit <- svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit ,dat)
# SVM Practice on the Gene Expression Dataset
library(e1071)
library(ISLR)
names(Khan)
# Let's examine the dimension of the data:
# This data set consists of expression measurements for 2,308 genes.
# The training and test sets consist of 63 and 20 observations respectively
dim(Khan$xtrain )
dim(Khan$xtest )
length(Khan$ytrain )
length(Khan$ytest )
table(Khan$ytrain )
table(Khan$ytest )
# We will use a support vector approach to predict cancer subtype using gene expression measurements.
# In this data set, there are a very large number of features relative to the number of observations.
# This suggests that we should use a linear kernel, because the additional flexibility that will
# result from using a polynomial or radial kernel is unnecessary.
dat <- data.frame(x=Khan$xtrain , y = as.factor(Khan$ytrain ))
out <- svm(y ~., data=dat, kernel="linear",cost=10)
summary(out)
dat.te=data.frame(x=Khan$xtest , y = as.factor(Khan$ytest ))
pred.te=predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'),col=seq(2),pch=1,text.col=seq(2))
#
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c('Positive Train','Positive Test','Negative Train','Negative Test'),col=c(1,1,2,2), pch=c(1,2,1,2), text.col=c(1,1,2,2))
library(e1071)
library(rpart)
data(Ozone)
library(plyr)
data(Ozone)
data(ozone)
index <- 1:nrow(Ozone)
testindex <- sample(index, trunc(length(index)/3))
index <- 1:nrow(ozone)
testindex <- sample(index, trunc(length(index)/3))
testset <- na.omit(ozone[testindex,-3])
attach(ozone)
data(ozone)
attach(ozone)
attach(data)
data <- data(ozone)
attach(data)
library(e1071)
library(rpart)
library(plyr)
data <- data(ozone)
attach(data)
data(ozone)
library(e1071)
library(rpart)
library(plyr)
data(ozone)
# split data into a train and test set
index <- 1:nrow(ozone)
testindex <- sample(index, trunc(length(index)/3))
testset <- na.omit(ozone[testindex,-3])
testset <- na.omit(ozone[testindex,-2])
attach(ozone)
testset <- na.omit(ozone[testindex,:])
testset <- na.omit(ozone[testindex,-1])
testset <- na.omit(ozone[testindex])
trainset <- na.omit(ozone[-testindex])
svm.model <- svm(V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)
data.frame(data(ozone))
attach(ozone)
attach(ozone_data)
ozone_data <- data.frame(data(ozone))
attach(ozone_data)
ozone_data <- data.frame(ozone)
attach(ozone_data)
# split data into a train and test set
index <- 1:nrow(ozone)
# split data into a train and test set
index <- 1:nrow(ozone_data)
testindex <- sample(index, trunc(length(index)/3))
testset <- na.omit(ozone_data[testindex,-3])
trainset <- na.omit(ozone_data[-testindex,-3])
svm.model <- svm(V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)
data(iris)
attach(iris)
## classification mode
# default with factor response:
model <- svm(Species ~ ., data = iris)
# alternatively the traditional interface:
x <- subset(iris, select = -Species)
y <- Species
model <- svm(x, y)
print(model)
summary(model)
# test with train data
pred <- predict(model, x)
# (same as:)
pred <- fitted(model)
# Check accuracy:
table(pred, y)
# compute decision values and probabilities:
pred <- predict(model, x, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]
# visualize (classes by color, SV by crosses):
plot(cmdscale(dist(iris[,-5])),
col = as.integer(iris[,5]),
pch = c("o","+")[1:150 %in% model$index + 1])
# create data
x <- seq(0.1, 5, by = 0.05)
y <- log(x) + rnorm(x, sd = 0.2)
# estimate model and predict input values
m   <- svm(x, y)
new <- predict(m, x)
# visualize
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)
# create 2-dim. normal with rho=0:
X <- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)
# traditional way:
m <- svm(X, gamma = 0.1)
# formula interface:
m <- svm(~., data = X, gamma = 0.1)
# or:
m <- svm(~ a + b, gamma = 0.1)
# test:
newdata <- data.frame(a = c(0, 4), b = c(0, 4))
predict (m, newdata)
# visualize:
plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)
# weights: (example not particularly sensible)
i2 <- iris
levels(i2$Species)[3] <- "versicolor"
summary(i2$Species)
wts <- 100 / table(i2$Species)
wts
m <- svm(Species ~ ., data = i2, class.weights = wts)
## example using the promotergene data set
data(promotergene)
library(kernlab)
packages.install('kernlab')
install.packages('kernlab')
library(kernlab)
## example using the promotergene data set
data(promotergene)
## create test and training set
ind <- sample(1:dim(promotergene)[1],20)
genetrain <- promotergene[-ind, ]
genetest <- promotergene[ind, ]
## train a support vector machine
gene <-  ksvm(Class~.,data=genetrain,kernel="rbfdot",\
kpar=list(sigma=0.015),C=70,cross=4,prob.model=TRUE)
## train a support vector machine
gene <-  ksvm(Class~.,data=genetrain,kernel="rbfdot",
kpar=list(sigma=0.015),C=70,cross=4,prob.model=TRUE)
## predict gene type probabilities on the test set
genetype <- predict(gene,genetest,type="probabilities")
library(e1071)
m1 <- matrix( c(
0,    0,    0,    1,    1,    2,     1, 2,    3,    2,    3, 3, 0, 1,2,3,
0, 1, 2, 3,
1,    2,    3,    2,    3,    3,     0, 0,    0,    1, 1, 2, 4, 4,4,4,    0,
1, 2, 3,
1,    1,    1,    1,    1,    1,    -1,-1,  -1,-1,-1,-1, 1 ,1,1,1,     1,
1,-1,-1
), ncol = 3 )
Y = m1[,3]
X = m1[,1:2]
df = data.frame( X , Y )
par(mfcol=c(4,2))
for( cost in c( 1e-3 ,1e-2 ,1e-1, 1e0,  1e+1, 1e+2 ,1e+3)) {
#cost <- 1
model.svm <- svm( Y ~ . , data = df ,  type = "C-classification" , kernel =
"linear", cost = cost,
scale =FALSE )
#print(model.svm$SV)
plot(x=0,ylim=c(0,5), xlim=c(0,3),main= paste( "cost: ",cost, "#SV: ",
nrow(model.svm$SV) ))
points(m1[m1[,3]>0,1], m1[m1[,3]>0,2], pch=3, col="green")
points(m1[m1[,3]<0,1], m1[m1[,3]<0,2], pch=4, col="blue")
points(model.svm$SV[,1],model.svm$SV[,2], pch=18 , col = "red")
}
data(spam)
## create test and training set
index <- sample(1:dim(spam)[1])
spamtrain <- spam[index[1:floor(dim(spam)[1]/2)], ]
spamtest <- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]
## train a support vector machine
filter <- ksvm(type~.,data=spamtrain,kernel="rbfdot",
kpar=list(sigma=0.05),C=5,cross=3)
filter
## predict mail type on the test set
mailtype <- predict(filter,spamtest[,-58])
## Check results
table(mailtype,spamtest[,58])
>>>>>>> 3d531c7... Add new files for SVM Labs
